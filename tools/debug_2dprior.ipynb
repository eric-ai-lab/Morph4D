{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ControlNet Direct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773872c2d6444303baf8bb2a4b5530d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from lib_prior.diffusion.ctrl_sd_prior import CtrSDPrior\n",
    "\n",
    "prior = CtrSDPrior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imageio\n",
    "# from matplotlib import pyplot as plt\n",
    "# import torch, numpy as np\n",
    "\n",
    "# img_fn = \"./data/DAVIS/train/images/00000.jpg\"\n",
    "# mask_fn = \"./data/DAVIS/train/epipolar_mask/00000.jpg\"\n",
    "\n",
    "# img_np = imageio.imread(img_fn)\n",
    "# img_np = img_np.astype(np.float32) / 255.0\n",
    "# mask_np = 1.0 - imageio.imread(mask_fn).astype(np.float32) / 255.0\n",
    "# L = (img_np.shape[1] - img_np.shape[0]) //2\n",
    "# img_np = img_np[:, L:L+img_np.shape[0], :]\n",
    "# mask_np = mask_np[:, L:L+img_np.shape[0]]\n",
    "# # plt.imshow(mask_np)\n",
    "# print(mask_np.shape)\n",
    "# print(mask_np.max())\n",
    "# print(img_np.shape)\n",
    "\n",
    "# mask_np = np.random.rand(*mask_np.shape).reshape(*mask_np.shape) > 0.4\n",
    "\n",
    "# inpainted = prior.inpaint(\n",
    "#     img_np,\n",
    "#     # np.zeros_like(mask_np),\n",
    "#     mask_np,\n",
    "#     \"realistic and clear crop of a photo\",\n",
    "#     diff_steps=50,\n",
    "# )\n",
    "\n",
    "# fig = plt.figure(figsize=(12, 4))\n",
    "# plt.subplot(1, 3, 1)\n",
    "# plt.imshow(img_np), plt.title(\"Input\"), plt.axis(\"off\")\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.imshow(mask_np), plt.title(\"Mask\"), plt.axis(\"off\")\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.imshow(inpainted), plt.title(\"Output\"), plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| endurance | INFO | Jan-18-19:32:58 | Loading motion masks...   [solver_utils.py:117]\n",
      "100%|██████████| 80/80 [00:00<00:00, 2353.16it/s]\n",
      "| endurance | INFO | Jan-18-19:32:58 | Loading motion masks...   [solver_utils.py:117]\n",
      "100%|██████████| 80/80 [00:00<00:00, 3486.25it/s]\n",
      "| endurance | INFO | Jan-18-19:32:58 | Loading flows...   [solver_utils.py:58]\n",
      "100%|██████████| 79/79 [00:00<00:00, 294.86it/s]\n",
      "| endurance | INFO | Jan-18-19:32:58 | Loading rgbs...   [solver_utils.py:42]\n",
      "100%|██████████| 80/80 [00:00<00:00, 268.77it/s]\n",
      "| endurance | INFO | Jan-18-19:32:59 | Loading depths from ./data/DAVIS/train/zoe_depth ...   [solver_utils.py:102]\n",
      "100%|██████████| 80/80 [00:00<00:00, 130.70it/s]\n",
      "| endurance | INFO | Jan-18-19:32:59 | rgbs: (80, 480, 854, 3), depths: (80, 480, 854), flows: [158,(480, 854, 2)], motion_masks: (80, 480, 854)   [solver_utils.py:188]\n",
      "| endurance | INFO | Jan-18-19:32:59 | Filtering depth maps...   [solver_utils.py:229]\n",
      "100%|██████████| 80/80 [00:00<00:00, 384.88it/s]\n",
      "| endurance | INFO | Jan-18-19:33:01 | rounding the flow ...   [prior2d.py:114]\n",
      "100%|██████████| 158/158 [00:00<00:00, 573.81it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os, os.path as osp\n",
    "import logging\n",
    "from lib_4d.solver_gs import Solver\n",
    "from lib_4d.gs_static_model import StaticGaussian\n",
    "from lib_4d.gs_dyn_model import DenseDynGaussian\n",
    "from lib_4d.camera import SimpleFovCameras\n",
    "from lib_4d.cfg_helpers import OptimCFG, GSControlCFG\n",
    "from lib_prior.diffusion.sd_sds import StableDiffusionSDS\n",
    "\n",
    "# load and render a novel view\n",
    "src = \"./data/DAVIS/train\"\n",
    "static_saved_dir = \"data/DAVIS/train/log/20240117_143638\"\n",
    "dyn_saved_dir = \"data/DAVIS/train/log/20240117_144846\"\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "solver = Solver(\n",
    "    src,\n",
    "    device,\n",
    "    # start_frame=35,\n",
    "    # end_frame=55,\n",
    "    depth_mode=\"zoe\",  # \"marigold\"\n",
    "    align_to_zoe=True,\n",
    "    load_resize=1.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| endurance | INFO | Jan-18-19:33:01 | Loading static model from data/DAVIS/train/log/20240117_143638/joint_model.pth   [gs_static_model.py:68]\n",
      "| endurance | INFO | Jan-18-19:33:01 | StaticGaussian: 218.3K points   [gs_static_model.py:109]\n",
      "| endurance | INFO | Jan-18-19:33:01 | _xyz, 0.655M   [gs_static_model.py:112]\n",
      "| endurance | INFO | Jan-18-19:33:01 | _features_dc, 0.655M   [gs_static_model.py:112]\n",
      "| endurance | INFO | Jan-18-19:33:01 | _features_rest, 0.000M   [gs_static_model.py:112]\n",
      "| endurance | INFO | Jan-18-19:33:01 | _opacity, 0.218M   [gs_static_model.py:112]\n",
      "| endurance | INFO | Jan-18-19:33:01 | _scaling, 0.655M   [gs_static_model.py:112]\n",
      "| endurance | INFO | Jan-18-19:33:01 | _rotation, 0.873M   [gs_static_model.py:112]\n",
      "| endurance | INFO | Jan-18-19:33:01 | ------------------------------   [gs_static_model.py:113]\n",
      "| endurance | INFO | Jan-18-19:33:01 | Loading dynamic model from data/DAVIS/train/log/20240117_144846/joint_dyn_model_1.0res.pth   [gs_dyn_model.py:75]\n",
      "| endurance | INFO | Jan-18-19:33:01 | DenseDynGaussian: 28.6K points and 80 time step   [gs_dyn_model.py:119]\n",
      "| endurance | INFO | Jan-18-19:33:01 | _xyz, 6.872M   [gs_dyn_model.py:124]\n",
      "| endurance | INFO | Jan-18-19:33:01 | _features_dc, 0.086M   [gs_dyn_model.py:124]\n",
      "| endurance | INFO | Jan-18-19:33:01 | _features_rest, 0.000M   [gs_dyn_model.py:124]\n",
      "| endurance | INFO | Jan-18-19:33:01 | _opacity, 0.029M   [gs_dyn_model.py:124]\n",
      "| endurance | INFO | Jan-18-19:33:01 | _scaling, 0.086M   [gs_dyn_model.py:124]\n",
      "| endurance | INFO | Jan-18-19:33:01 | _rotation, 9.163M   [gs_dyn_model.py:124]\n",
      "| endurance | INFO | Jan-18-19:33:01 | ------------------------------   [gs_dyn_model.py:125]\n"
     ]
    }
   ],
   "source": [
    "model_fn = osp.join(static_saved_dir, \"joint_model.pth\")\n",
    "cam_fn = osp.join(static_saved_dir, \"joint_cams.pth\")\n",
    "st_model = StaticGaussian(load_fn=model_fn).to(solver.device)\n",
    "cams: SimpleFovCameras = SimpleFovCameras(solver.T, 40.0)\n",
    "cams.load_state_dict(torch.load(cam_fn), strict=True)\n",
    "cams.to(solver.device)\n",
    "dyn_model_fn = osp.join(dyn_saved_dir, \"joint_dyn_model_1.0res.pth\")\n",
    "dyn_model = DenseDynGaussian(load_fn=dyn_model_fn).to(solver.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lib_4d.gs_viz_helpers import viz_scene, cat_gs\n",
    "# from pytorch3d.transforms import quaternion_to_matrix\n",
    "# from lib_4d.solver_utils import fovdeg2focal\n",
    "\n",
    "\n",
    "# res=  512\n",
    "# # gs5_param = dyn_model(40)\n",
    "# # gs5_param = cat_gs(*gs5_param, *st_model())\n",
    "# gs5_param = st_model()\n",
    "\n",
    "# start_tid = 0\n",
    "# end_tid = cams.T\n",
    "\n",
    "# viz_R = quaternion_to_matrix(cams.q_wc[start_tid : end_tid + 1])\n",
    "# viz_t = cams.t_wc[start_tid : end_tid + 1]\n",
    "# viz_f = fovdeg2focal(90.0)\n",
    "# _, viz_dict = viz_scene(\n",
    "#     res, res, viz_R, viz_t, viz_f=viz_f, gs5_param=gs5_param, draw_camera_frames=True, return_full=True\n",
    "# )\n",
    "# print(viz_dict.keys())\n",
    "# render_dict = viz_dict[\"scene_camera_20deg\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7251649305555555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37ecbda68e74860a37d2d7e8da5b6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "with torch.no_grad():\n",
    "    # sample a camera\n",
    "    left_R_wc, left_t_wc = cams.get_ith_Rt_wc(0)\n",
    "    left_R_wc = left_R_wc.detach().clone()\n",
    "    left_t_wc = left_t_wc.detach().clone()\n",
    "    left_t_wc[0] = left_t_wc[0] + 1.5\n",
    "    left_t_wc[2] = left_t_wc[2] -0.2\n",
    "    \n",
    "    # TODO: smart sample\n",
    "    \n",
    "    render_dict = solver.render_frame(\n",
    "        cams.rel_focal,\n",
    "        cams,\n",
    "        ind=0,\n",
    "        static_model=st_model,\n",
    "        dynamic_model=dyn_model,\n",
    "        render_view_id=0,\n",
    "        square=True,\n",
    "        R_cw=left_R_wc.T,\n",
    "        t_cw=-left_R_wc.T @ left_t_wc,\n",
    "        scale_factor=0.00001,\n",
    "        opa_factor=1.0\n",
    "    )\n",
    "    \n",
    "    render_mask_np = (render_dict[\"alpha\"][0] < 0.5).cpu().numpy()\n",
    "    \n",
    "    render_dict = solver.render_frame(\n",
    "        cams.rel_focal,\n",
    "        cams,\n",
    "        ind=0,\n",
    "        static_model=st_model,\n",
    "        dynamic_model=dyn_model,\n",
    "        render_view_id=0,\n",
    "        square=True,\n",
    "        R_cw=left_R_wc.T,\n",
    "        t_cw=-left_R_wc.T @ left_t_wc,\n",
    "        # scale_factor=0.00001,\n",
    "        # opa_factor=1.0\n",
    "    )\n",
    "\n",
    "rgb = render_dict[\"rgb\"]\n",
    "alpha = render_dict[\"alpha\"]\n",
    "# render_mask_np = (alpha[0] < 0.5).cpu().numpy()\n",
    "\n",
    "dksize = 2\n",
    "dilate_kernel = np.ones((dksize, dksize), np.uint8)\n",
    "# render_mask_np = cv2.dilate(\n",
    "#     render_mask_np.astype(np.uint8), dilate_kernel, iterations=1\n",
    "# )\n",
    "render_mask_np = cv2.erode(\n",
    "    render_mask_np.astype(np.uint8), dilate_kernel, iterations=1\n",
    ")\n",
    "\n",
    "random_mask_np = (\n",
    "    np.random.rand(*render_mask_np.shape).reshape(*render_mask_np.shape) > 2.0\n",
    ")\n",
    "# random_mask_np = cv2.dilate(\n",
    "#     random_mask_np.astype(np.uint8), dilate_kernel, iterations=1\n",
    "# )\n",
    "mask_np = np.logical_or(render_mask_np, random_mask_np)\n",
    "\n",
    "\n",
    "img_np = rgb.permute(1, 2, 0).cpu().numpy()\n",
    "img_np = np.clip(img_np, 0.0, 1.0)\n",
    "\n",
    "print(mask_np.mean())\n",
    "\n",
    "inpainted = prior.inpaint(\n",
    "    img_np,\n",
    "    # np.ones_like(mask_np),\n",
    "    # np.zeros_like(mask_np),\n",
    "    mask_np,\n",
    "    \"smooth and realistic\",\n",
    "    # \"realistic, sharp and clear crop of a photo\",\n",
    "    # \"railway, village\",\n",
    "    diff_steps=100,\n",
    ")\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(img_np), plt.title(\"Input\"), plt.axis(\"off\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(mask_np), plt.title(\"Mask\"), plt.axis(\"off\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(inpainted), plt.title(\"Output\"), plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"test.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError(\"stop here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SD SDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from lib_prior.diffusion.sd_sds import StableDiffusionSDS\n",
    "\n",
    "prior = StableDiffusionSDS(\n",
    "    torch.device(\"cuda:0\"), fp16=True, vram_O=True, t_range=[0.8,0.99]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from matplotlib import pyplot as plt\n",
    "import torch, numpy as np\n",
    "\n",
    "img_fn = \"./data/DAVIS/train/images/00000.jpg\"\n",
    "mask_fn = \"./data/DAVIS/train/epipolar_mask/00000.jpg\"\n",
    "\n",
    "img_np = imageio.imread(img_fn)\n",
    "img_np = img_np.astype(np.float32) / 255.0\n",
    "mask_np = 1.0 - imageio.imread(mask_fn).astype(np.float32) / 255.0\n",
    "L = (img_np.shape[1] - img_np.shape[0]) //2\n",
    "img_np = img_np[:, L:L+img_np.shape[0], :]\n",
    "mask_np = mask_np[:, L:L+img_np.shape[0]]\n",
    "# plt.imshow(mask_np)\n",
    "print(mask_np.shape)\n",
    "print(mask_np.max())\n",
    "print(img_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior.get_text_embeds(\n",
    "    [\"a train is turning on the two side rails in a farm field\"],\n",
    "    [\n",
    "        \"ugly, bad anatomy, blurry, pixelated obscure, unnatural colors, poor lighting, dull, and unclear, lowres, low quality, artifacts, duplicate, morbid, mutilated, poorly drawn face, dehydrated, bad proportions\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rgb = torch.from_numpy(img_np).permute(2, 0, 1).unsqueeze(0).cuda()\n",
    "prior.train_step(pred_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the trained model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vid24d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
